ML Notes

Lexalytics
For Supervised Learning
Tokenization
	Use white space to define word
Pats of Speech Tagging
	Recognize Named Entities, extract themes and process sentiment
Named Entity Recognition
	Main use of POS
Sentiment
	
Classification

For Unsupervised Learning
Concept Matrix 
	whats closed to or not
Syntax Matrix
	the most likely parsing of a sentence
In a sentence 
	–


Hybrid model
Every ML method is aligned with patterns and rules of the language. 

Low-Level:
Tokenization: ML + Rules 
PoS Tagging: Machine Learning 
Chunking: Rules 
Sentence Boundaries: ML + Rules 
Syntax Analysis: ML + Rules 
Mid-Level:
Entities: ML + Rules to determine “Who, What, Where” 
Themes: Rules “What’s the buzz?” 
Topics: ML + Rules “About this?” 
Summaries: Rules “Make it short” 
Intentions: ML + Rules “What are you going to do?” 
Intentions uses the syntax matrix to extract the intender, intendee, and intent 
We use ML to train models for the different types of intent 
We use rules to whitelist or blacklist certain words 
Multilayered approach to get you the best accuracy 
High-Level:
Apply Sentiment: ML + Rules “How do you feel about that?



Notes on Machine Learning Fundamentals

10-12 hours per week

Use of nearest neighbour with 3.08% error rate
to find handwritter numbers
mentioned of training data as 10-cross-validation
(=divide training data)

use of structures to reduce k-NN search

distance functions
Lp norms
Metric spaces

	
Lp norms
	L1 = (manhatan distance)
	L2 = eucledian distance
	Linfinty = max point distance

Metric functions 
	edit distance
		number of insertions, deletions, substitutions to get from x to y

Kullback-Leibler divergence, relative entropy (probability vectors)
	d(p,q) = Sum (p(x)log(p(x)/q(x)))

Prediction Problem
	3 types of output space
	discrete 
		binary 
		multiclass
		structured 
	continuous
		regression problems
	probability values


Roadmap 
	Solving rediction problems
		Classification, regression, probability estimation
	Representaion learning
		Clustering,projection, dictionary learning, autoencoders
	Deep Learning



Quiz
Problem 15
A bank decides to use nearest neighbor classification to decide which clients to offer a certain investment option. It has a database of clients that were already offered this product, along with information about whether these clients accepted or declined. This is the training set. It also has a long list of other clients who have not yet been offered this product; it wants to choose clients that are reasonably likely to accept, and will do so by using nearest neighbor using the training set.

Suppose the following information is available on each client:

age
annual income
amount in bank
zip code
driver license number

Which of these features do you think would be most relevant to the classification problem? Would it make sense to use Euclidean distance, or would something else be better?

Most relevan features are annual income, amount in bank and age. To eliminate the differences in range it is wise to use to divide each distance with max feature (x-y)/max(x,y)

Problem 16
How might nearest neighbor be used in a recommender system? Suppose a movie streaming service keeps track of which movies its users watch and what their ratings are. Is there a way to use this information to make movie recommendations to users? What would the data space be, and what kind of distance function would be suitable?

-The data space would be discret and the output probability values. The distance function would be Kullback-Leibler divergence

-Recommended answer: See if there common movies between others and there ratings. In this case we use the edit distance (number of insertions, deletions, substitutions to get from x to y)